---
title: "Gestión de Riesgos Financieros"
author: "Stephanía Mosquera López"
date: "2/07/2022"
output:
    html_document:
      toc: true
      toc_float: true
      toc_depth: 3
---

# Riesgo de Mercado

## Retornos Financieros

Este documento es un [R Markdown](http://rmarkdown.rstudio.com) Notebook y será el formato en el que tendremos las notas de clase. 

Vamos a arrancar descargando e instalando `R` siguiendo estas instrucciones:

1. Descargar R de la página web http://cran.r-project.org/.
2. Aquí pueden descargar `R` para Linux, Mac o Windows.
3. Una vez descargada la versión más actualizada del software, ejecute el instalador. 

Después de que esté instalado `R`, continuamos con la instalación de `RStudio`, que es la interfraz gráfica que vamos a utilizar. Las instrucciones son:

1. Descargar RStudio de la página web https://www.rstudio.com/products/RStudio/.
2. Dar click en la pestaña *Products*,`RStudio`. 
3. Descargar *RStudio Desktop*, Open Source Edition, que es la versión gratuita. 

Una vez instalado `RStudio` vamos a instalar `R Markdown`, que es una librería que les permitirá crear documentos como este, donde pueden comentar de una manera más organizada los códigos que escriban. También pueden crear páginas web, dashboards, PDFs, documentos de Word, entre otros.  

En la consola escriba:

```{r warning=FALSE}
#install.packages("rmarkdown")
library(rmarkdown)
```

En la página web  http://rmarkdown.rstudio.com/gallery.html pueden encontrar ejemplos y templates de documentación realizada con `R Markdown.

### Obtener información de FRED Data

La Reserva Federal del Banco de St.Lois mantiene la base de datos pública llamada **FRED**, que contiene información de:

- Variables macroeconómicas: PIB, desempleo, oferta de dinero, ...
- Tasas de cambio
- Tasas de interés
- Precios de commodities
- Índices accionarios

Se puede descargar la información desde la página web https://fred.stlouisfed.org/ o directamente desde `R`, así:

```{r warning=FALSE,message=FALSE}
library(quantmod)
SP <- getSymbols("SP500",src="FRED", auto.assign=FALSE)
SP <- na.omit(SP)
```
Para saber si cargamos la información adecuada podemos ver algunas líneas de los datos.

**Del inicio:**
```{r}
head(SP,4)
```
**Del final:**
```{r}
tail(SP,4)
```

Ahora grafiquemos el índice: 
```{r warning=TRUE}
plot(SP)
```

### Cálculo de los Retornos

El **retorno discreto** es igual a:

$ret_{t}=\frac{SP_{t}-SP_{t-1}}{SP_{t-1}}$

Mientras que el **retorno continuo** lo definimos como:

$logret_{t}=log(1+ret_{t})=log(SP_{t})-log(SP_{t-1})=log(\frac{SP_{t}}{SP_{t-1}})$

```{r}
logret <-diff(log(SP))
logret <-na.omit(logret)
head(logret,4)
```
**Gráfica de los retornos logarítmicos** 

```{r warning=TRUE}
plot(logret)
```

Si queremos pasar del retorno logarítmico al retorno discreto lo podemos hacer así: $ret_{t}=exp(logret_{t})-1$

```{r}
ret <-exp(logret)-1
head(ret,4)
```
**Pregunta: ¿Qué tan parecidos son los retornos discretos con los logarítmicos? ¿Por qué preferimos los logarítmicos?**

**Horizontes de tiempo más largos**

También podemos calcular los retornos para horizontes de tiempo diferentes al diario. 

**De retorno logarítmico a 1-día a retorno logarítmico a n-días:**

- Empiece con el retorno logarítmico  a 1-día, de $t-1$ a $t$:$logret_{t}$
- Calcule el retorno logarítmico a n-días, de $t-n$ a $t$: $logret_{t}+...+logret_{t-n+1}$
- Calcule el retorno discreto a n-días: $exp(logret_{t}+...+logret_{t-n+1})-1$

**Estimación de los retornos semanales**

```{r}
logret_w <- apply.weekly(logret,sum)
head(logret_w,4)
```

`R` también tiene funciones para:

- Retornos mensuales:    `apply.monthly`
- Retornos trimestrales: `apply.quarterly`
- Retornos anuales:      `apply.yearly`     

### Hechos Estilizados

**1. Baja autocorrelación de los retornos diarios**

```{r}
pacf(logret)
```

**2. La distribución de los retornos no es normal**

```{r}
hist(logret, freq=FALSE,breaks=20, col="gray")
lines(seq(-0.1,0.05, by=0.001), dnorm(seq(-0.1,0.05, by=0.001),
      mean(logret), sd(logret)), col="blue")
```
**3. Clusters de Volatilidad**

```{r warning=TRUE}
ret2 = logret^2
pacf(ret2)

```

## Repaso Estadística

Primero cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Ahora vamos a descargar el S&P 500, las primeras 10 acciones del índice con mayor capitalización bursátil a enero de 2022 y un bono soberano estadounidense: 

```{r warning=FALSE, message=FALSE}
#S&P500
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp1<-na.locf(GSPC$GSPC.Adjusted)  

#1.Apple
getSymbols("AAPL",src='yahoo', from= startt , to = endd);sp2<-na.locf(AAPL$AAPL.Adjusted) 

#2.Microsoft
getSymbols("MSFT",src='yahoo', from= startt , to = endd); sp3<-na.locf(MSFT$MSFT.Adjusted)  

#3.Amazon
getSymbols("AMZN",src='yahoo', from= startt , to = endd); sp4<-na.locf(AMZN$AMZN.Adjusted)  

#4.Alphabet Class A
getSymbols("GOOGL",src='yahoo', from= startt , to = endd); sp5<-na.locf(GOOGL$GOOGL.Adjusted)

#5.Tesla
getSymbols("TSLA",src='yahoo', from= startt , to = endd); sp6<-na.locf(TSLA$TSLA.Adjusted) 

#6.Alphabet Class C
getSymbols("GOOG",src='yahoo', from= startt , to = endd); sp7<-na.locf(GOOG$GOOG.Adjusted)

#7.Meta
getSymbols("FB",src='yahoo', from= startt , to = endd); sp8<-na.locf(FB$FB.Adjusted) 

#8.NVIDIA Corporation
getSymbols("NVDA",src='yahoo', from= startt , to = endd); sp9<-na.locf(NVDA$NVDA.Adjusted)

#9.Berkshire Hathaway Inc
getSymbols("BRK-B",src='yahoo', from= startt , to = endd); sp10<-na.locf(`BRK-B`$`BRK-B.Adjusted`) 

#UnitedHealth Group
getSymbols("UNH",src='yahoo', from= startt , to = endd); sp11<-na.locf(UNH$UNH.Adjusted)

#Tbill 13 weeks
getSymbols("^IRX",src='yahoo', from= startt , to = endd); sp12<-na.locf(IRX$IRX.Adjusted)     
```
En la siguiente parte unimos todas las series en una misma matriz:

```{r warning=FALSE}
sp <- merge(sp1,sp2,fill=na.locf)
sp <- merge(sp,sp3,fill=na.locf)
sp <- merge(sp,sp4,fill=na.locf)
sp <- merge(sp,sp5,fill=na.locf)
sp <- merge(sp,sp6,fill=na.locf)
sp <- merge(sp,sp7,fill=na.locf)
sp <- merge(sp,sp8,fill=na.locf)
sp <- merge(sp,sp9,fill=na.locf)
sp <- merge(sp,sp10,fill=na.locf)
sp <- merge(sp,sp11,fill=na.locf)
sp <- merge(sp,sp12,fill=na.locf)

sm<-sp
```
### Precios en Niveles

**Gráfica de los Precios en Niveles:**

```{r, warning=FALSE, message=FALSE}

par(mfrow=c(3,1))
plot(sm[,1], main ="SP500")
plot(sm[,2], main ="Apple")
plot(sm[,3], main ="Microsoft")
plot(sm[,4], main ="Amazon")
plot(sm[,5], main ="Alphabet Class A")
plot(sm[,6], main ="Tesla")
plot(sm[,7], main ="Alphabet Class C")
plot(sm[,8], main ="Meta")
plot(sm[,9], main ="NVIDIA")
plot(sm[,10], main ="Berkshire Hathaway")
plot(sm[,11], main ="United Health Group")
plot(sm[,12], main ="Tbill 13 weeks")

```
Calculemos las estadísticas descriptivas de los precios:

```{r warning=FALSE}
m <- ncol(sm)
EstDesPrecios <- matrix(,7,m)

for (i in 1:m){
  EstDesPrecios[1,i]=colMins(sm[,i])
  EstDesPrecios[2,i]=colMeans(sm[,i])
  EstDesPrecios[3,i]=colMaxs(sm[,i])
  EstDesPrecios[4,i]=colVars(sm[,i])
  EstDesPrecios[5,i]=colSds(sm[,i])
  EstDesPrecios[6,i]=colSkewness(sm[,i])
  EstDesPrecios[7,i]=colKurtosis(sm[,i])
}

rownames(EstDesPrecios) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis")
colnames(EstDesPrecios) <- c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

EstDesPrecios %>%
  kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```

### Retornos

```{r, warning=FALSE, message = FALSE}
ret<- diff(log(sm))*100
ret<-na.locf(ret)
ret[is.na(ret)] <- 0

```

**Gráfica de los Retornos:**

```{r}

par(mfrow=c(3,1))
plot(ret[,1], main ="SP500")
plot(ret[,2], main ="Apple")
plot(ret[,3], main ="Microsoft")
plot(ret[,4], main ="Amazon")
plot(ret[,5], main ="AlphabetA")
plot(ret[,6], main ="Tesla")
plot(ret[,7], main ="AlphabetB")
plot(ret[,8], main ="Meta")
plot(ret[,9], main ="NVIDIA")
plot(ret[,10], main ="Berkshire Hathaway")
plot(ret[,11], main ="United Health Group")
plot(ret[,12], main ="Tbill 13 weeks")

```
Calculemos ahora las estadísticas descriptivas de los retornos:

```{r warning=FALSE}
m <- ncol(ret)
EstDesRetornos <- matrix(,8,12)

for (i in 1:m){
  EstDesRetornos[1,i]=colMins(ret[,i])
  EstDesRetornos[2,i]=colMeans(ret[,i])
  EstDesRetornos[3,i]=colMaxs(ret[,i])
  EstDesRetornos[4,i]=colVars(ret[,i])
  EstDesRetornos[5,i]=colSds(ret[,i])
  EstDesRetornos[6,i]=colSkewness(ret[,i])
  EstDesRetornos[7,i]=colKurtosis(ret[,i])
  EstDesRetornos[8,i]=jarque.bera.test(ret[,i])$p.value
}

rownames(EstDesRetornos) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis","Jarque Bera")

colnames(EstDesRetornos) <- c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

EstDesRetornos %>%
  kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```
### Modelo CAPM

El Modelo de Valoración de Activos de Capital (CAPM) es un modelo que proporciona una explicación de la prima por riesgo, $\mu_{j}-r{0}$, de cada activo. 

Es una extensión del modelo de media-varianza para un solo inversionista al mercado completo. 

Es un modelo de equilibrio en el sentido tradicional: oferta = demanda. 

**Ecuación del CAPM:**

$\mu_{j}=r_{0}+\beta_{j}(\mu_{M}-r_{0})$ para $j=1,2, ...,n$

Ahora procedemos a calcular los betas de las 10 acciones del SP500 con las que estamos trabajando:

```{r}
X<-as.matrix(ret[,1]-ret[,12])
YY<-ret[,2:11]
t<-dim(YY)[1]
n<-dim(YY)[2]
Y<-matrix(0,t,n)

for (i in 1:n){
  Y[,i]<-YY[,i]-ret[,12]}

Resultados <-matrix(0,n,4)

for (i in 1:n){
  Y2=Y[,i]
  olsreg <- lm(Y2~X)
  res <- summary(olsreg)
  Resultados[i,1:4] <- res$coef[2,1:4]}

rownames(Resultados) <- c("Beta Apple","Beta Microsoft","Beta Amazon","Beta Alphabet A","Beta Tesla","Beta AlphabetC","Beta Meta","Beta NVIDIA","Beta Berkshire Hathaway","Beta UnitedHealth")
colnames(Resultados) <- c("Estimate","Std.Error","t-value","p-value")

c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

Resultados %>%
  kable(digits=4) %>%
  kable_styling("striped",full_width = F, position = "center")
```

## Modelación Media

Primero cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Descargamos el SP500:

```{r}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
t <- nrow(sp)
date <- tk_index(sp)
```

### Paso 1. Estacionariedad

El primer paso de la metodología Box-Jenkins para ajustar un modelo ARMA a una serie de tiempo es establecer la **estacionariedad** de ésta.

Así, revisamos la estacionariedad de la serie utilizando la prueba de raíz unitaria de Dickey-Fuller aumentada:

```{r}
adf.test(sp)
```

Como la serie no es estacionaria debemos diferenciarla y realizar nuevamente la prueba:

```{r}
ret <-diff(log(sp))
ret <-na.omit(ret)

adf.test(ret)
```
Gráfica de los retornos logarítmicos: 

```{r}
plot(ret)
```

Una vez se realiza la inspección gráfica de los datos debemos calcular las estadísticas descriptivas de los datos y probar normalidad. 

```{r}
m <- ncol(ret)
EstDesRetornos <- matrix(,8,1)

  EstDesRetornos[1]=colMins(ret)
  EstDesRetornos[2]=colMeans(ret)
  EstDesRetornos[3]=colMaxs(ret)
  EstDesRetornos[4]=colVars(ret)
  EstDesRetornos[5]=colSds(ret)
  EstDesRetornos[6]=colSkewness(ret)
  EstDesRetornos[7]=colKurtosis(ret)
  EstDesRetornos[8]=jarque.bera.test(ret)$p.value
  
  rownames(EstDesRetornos) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis","Jarque Bera")

colnames(EstDesRetornos) <- c("SP500")

EstDesRetornos %>%
    kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```


```{r}
hist(sp, freq = F)
curve(dnorm(x, mean(sp), sd(sp)),
      col = "blue", lwd = 3, add = TRUE)
```

### Paso 2. Identificación

Procedemos a indentificar un modelo de la media condicional a los datos. Para estos nos basamos en la función de autocorrelación y la función de autocorrelación parcial.

```{r}
par(mfrow=c(2,1))
acf(ret, lag=24) 
pacf(ret, lag=24) 
```

También podemos realizar pruebas de autocorrelación:

```{r}
Box.test(ret,lag=12,type="Box-Pierce") #H0: no autocorrelación
Box.test(ret,lag=12,type="Ljung") #H0: no autocorrelación
```
### Paso 3. Especificación

El paso 3 consiste en especificar varios modelos y comparar mendiante criterios de selección. 

#### AIC

```{r}

AIC <-matrix(0,6,6)

d=0

for (p in 0:5)  
{
  for (q in 0:5)  
  {
    modelo<-arima(ret,order=c(p,d,q))
    AIC[p+1,q+1]<-AIC(modelo)
     } 
 } 

rownames(AIC) <- c("AR(0)","AR(1)","AR(2)","AR(3)","AR(4)","AR(5)")
colnames(AIC) <- c("MA(0)","MA(1)","MA(2)","MA(3)","MA(4)","MA(5)")

AIC %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```

#### BIC

```{r}

BIC <-matrix(0,6,6)

d=0

for (p in 0:5)  
{
  for (q in 0:5)  
  {
    modelo<-arima(ret,order=c(p,d,q))
    BIC[p+1,q+1]<-BIC(modelo)
     } 
 } 

rownames(BIC) <- c("AR(0)","AR(1)","AR(2)","AR(3)","AR(4)","AR(5)")
colnames(BIC) <- c("MA(0)","MA(1)","MA(2)","MA(3)","MA(4)","MA(5)")

BIC %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```
También podemos utilizar la función `auto.arima()` para escoger el mejor modelo. Esta función hace una combinación de pruebas de raíz unitaria (para saber el orden de integreación), minimización del criterio AIC y máximización del log de verosimilitud.

Podemos utilizar esta función para los precios en niveles o para los retornos. 

**Precios**

```{r}
auto.arima(sp, allowdrift = TRUE, allowmean = TRUE)
```
```{r}
modelo.precios = arima(sp, order=c(4,1,2))
coeftest(modelo.precios)

```

**Retornos**

```{r}
auto.arima(ret)
```

```{r}
modelo.retornos = arima(ret, order=c(2,0,2))
coeftest(modelo.retornos)

```

### Paso 4. Bondad de Ajuste

Debemos asegurarnos que los residuales no presenten autocorrelación y que sean homocedásticos. 

```{r}
res.precios <- residuals(modelo.precios)
res.retornos <- residuals(modelo.retornos)
```
**Precios Autocorrelación**

```{r}
par(mfrow=c(2,1)) 
acf(res.precios, lag=12) 
pacf(res.precios,lag=12)

Box.test(res.precios,lag=12,type="Box-Pierce") #H0: no autocorrelación
Box.test(res.precios,lag=12,type="Ljung") #H0: no autocorrelación
```

**Precios Heterocedasticidad**

```{r}
par(mfrow=c(2,1)) 
acf(res.precios^2, lag=12) 
pacf(res.precios^2,lag=12)

Box.test(res.precios^2,lag=12,type="Box-Pierce") #H0: no autocorrelación
Box.test(res.precios^2,lag=12,type="Ljung") #H0: no autocorrelación
```

**Retornos Autocorrelación**

```{r}
par(mfrow=c(2,1)) 
acf(res.retornos, lag=12) 
pacf(res.retornos,lag=12)

Box.test(res.retornos,lag=12,type="Box-Pierce") #H0: no autocorrelación
Box.test(res.retornos,lag=12,type="Ljung") #H0: no autocorrelación
```

**Retornos Heterocedasticidad**

```{r}
par(mfrow=c(2,1)) 
acf(res.retornos^2, lag=12) 
pacf(res.retornos^2,lag=12)

Box.test(res.retornos^2,lag=12,type="Box-Pierce") #H0: no autocorrelación
Box.test(res.retornos^2,lag=12,type="Ljung") #H0: no autocorrelación
```

### Paso 5. Pronosticar

**Precios**

```{r}
forecast<- predict(modelo.precios, n.ahead=50)

U <- forecast$pred + 1.96*forecast$se 
L <- forecast$pred - 1.96*forecast$se 
Y <- forecast$pred
fecha <- tk_index(sp)
fecha2 <- fecha[1188:1237]
sp2 <- as.matrix(sp)

plot(fecha,sp2,type="l",col="black")
lines(fecha2,U, col="red",lty=2)
lines(fecha2,L, col="red",lty=2)
lines(fecha2,Y, col="blue",type="l")

```

**Retornos**

```{r}
forecast<- predict(modelo.retornos, n.ahead=50)

U <- forecast$pred + 1.96*forecast$se 
L <- forecast$pred - 1.96*forecast$se 
Y <- forecast$pred
fecha <- tk_index(ret)
fecha2 <- fecha[1188:1237]
ret2 <- as.matrix(ret)

plot(fecha,ret2,type="l",col="black")
lines(fecha2,U, col="red",lty=2)
lines(fecha2,L, col="red",lty=2)
lines(fecha2,Y, col="blue",type="l")

```

## Modelación Varianza

Cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
library(rugarch)
library(qrmtools)
library(MTS)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')

```
Descargamos el SP500:

```{r message=FALSE}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
```
Calculamos los retornos logarítmicos:

```{r warning=FALSE}
ret <-diff(log(sp))
ret <-na.omit(ret)
```

### Volatilidad Histórica

Ahora procedemos a calcular la volatilidad histórica:

```{r warning=FALSE}
m=252
sp.volh= rollapply(ret,m,sd)
plot(sp.volh)
```

### Volatilidad EWMA

```{r}
ewma=EWMAvol(ret,lambda = 0.94)
sp.volRM = ewma[["Sigma.t"]]
plot(sp.volRM, type="l")
```

### Volatilidad GARCH

1.Estimamos el **modelo ARMA** de los retornos:

```{r}
auto.arima(ret)
media =arima(ret, order=c(2,0,4))
res = residuals(media)

Box.test(res,lag=12,type="Box-Pierce") 
Box.test(res,lag=12,type="Ljung") 
```
2. Después de extraer los residuales del modelo en media, realizamos pruebas de **heterocedasticidad**. 

La prueba de Engle para efectos ARCH tiene como hipótesis alterna que los errores al cuadrado están correlacionados con los errores al cuadrado rezagados.

$H_{a}:e_{t}^2=\alpha_{0}+\alpha_{1}e_{t-1}^2+...+\alpha_{m}e_{t-m}^2+u_{t}$

$H_{0}:\alpha_{0}=\alpha_{1}=...=\alpha_{m}=0$

```{r}
ArchTest(res, lag=12)

Box.test(res^2,lag=12,type="Box-Pierce") 
Box.test(res^2,lag=12,type="Ljung")

par(mfrow=c(2,1))
acf(res^2, lag=24) 
pacf(res^2, lag=24)
```

3. Estimación Modelo GARCH(1,1)

```{r}
Modelo <- garch(res,order=c(1,1))
summary(Modelo) 
```
Obtenemos el pronóstico de la varianza del modelo y estandarizamos los residuales del modelo en media:

```{r}
m <- nrow(ret)
res.garch <- fitted.values(Modelo)
res.garch <- res.garch[2:m,1]
res <- res[2:m]

res.est <- res/res.garch
```
4. Realizamos pruebas de bondad de ajuste sobre los nuevos residuales

```{r}
ArchTest(res.est, lag=12)

Box.test(res.est^2,lag=12,type="Box-Pierce") 
Box.test(res.est^2,lag=12,type="Ljung")

par(mfrow=c(2,1))
acf(res.est^2, lag=24) 
pacf(res.est^2, lag=24)
```

5. Por último, graficamos las tres varianzas que estimamos

```{r}
fecha <- tk_index(ret)
fecha <- fecha[2:m]

sp.volh2 <- as.matrix(sp.volh[2:m,1])
sp.volRM2 <- sqrt(sp.volRM[2:m,1])
sp.volGARCH <- res.garch

vol     <- matrix(,m-1,4)
vol[,1] <- sp.volh2
vol[,2] <- sp.volRM2
vol[,3] <- sp.volGARCH
vol[,4] <- as.matrix(sp[2:m,])

colnames(vol) <- c("VolHist", "VolRM", "VolGARCH", "SP500")
x= as.data.frame(vol)

graf <- ggplot(x,aes(x=fecha,y=VolHist)) + 
        geom_line(aes(y=VolHist), size = 0.25, col="red") +
        geom_line(aes(y=VolRM), size = 0.25, col="blue") +
        geom_line(aes(y=VolGARCH), size = 0.25, col="azure4") +             ylab('Volatilidad')+ theme_minimal() + theme(text = element_text(size=10), axis.text.x = element_text(size=10),axis.text.y = element_text(size=10))
        graf

```

## Estimación del VaR y ES

### Valor en Riesgo

El **Valor en Riesgo (VaR)** y el **Expected Shortfall (ES)** son medidas que buscan resumir en un solo número el riesgo total de un portafolio.

El **VaR** es una medida de la exposición de un portafolio de inversión al riesgo de mercado. Se difundió ampliamente después de su publicación en un documento técnico de **RiskMetrics** en 1994 para J.P. Morgan en Estados Unidos. Esta medida hoy constituye un paradigma financiero en el manejo de todo tipo de riesgos. El **VaR** se define como la **máxima pérdida** esperada (o la peor pérdida) que podría registrarse durante un determinado período de tiempo, para un nivel dado de confianza.

Formalmente, si se tiene un **nivel de significancia** $\alpha$, y si $q_{(1-\alpha)}$ es el **cuantil** $(1-\alpha)$  de la distribución de pérdidas para un **período de tiempo**, el VaR de un portafolio a ese nivel de confianza y en ese período de tiempo es igual a: 

$VaR_{(1-\alpha)}=q_{(1-\alpha)}(L)$, donde $L$ es la función de pérdidas. 

De esta manera, existen dos parámetros que determinan el **VaR** en principio: 

* **Período de tiempo:** Período durante el cual se mide la ganancia o pérdida del portafolio. Usualmente es a un día, 10 días, o un mes. 

* **Nivel de confianza:** Típicamente está entre 95% y 99%.

#### Limitaciones del VaR

El VaR es atractivo porque es fácil de entender. Nos dice qué tan mal pueden llegar a ser las cosas. Sin embargo, presenta las siguientes limitaciones:

1. Dice muy poco sobre los casos en las colas (pérdidas extremas).

2. Puede crear estructuras perversas de incentivos porque no se conoce la magnitud de las pérdidas que exceden las colas.

3. No es subaditivo, lo cual puede generar incentivos para construir portafolios demasiado concentrados.

### Expected Shortfall

El VaR no cumple con ser **subaditivo** (no cumple con este axioma de las medidas coherentes de riesgo), lo cual se suma a su pobre desempeño como medida de riesgo en situaciones extremas.Una alternativa teórica para la estimación del riesgo es el método de **pérdidas esperadas en las colas (ES)**. Se le conoce como el expected shortfall o VaR condicional. 

Se define el **ES** como el valor esperado de las pérdidas, 𝐿, en caso tal que éstas sean superiores al **VaR**: $ES=E[L|L>VaR]$. Las formas de calcular este valor esperado difieren según el método utilizado para calcular el **VaR subyacente**.

A continuación veremos cuatro **metodologías** para calcular el **VaR** y **ES**.

### Método Simulación Histórica

El método de simulación histórica (HS por sus siglas en inglés) es un método no paramétrico, pues no supone que los datos se distribuyen con una función de probabilidad teórica específica, sino que aproxima el VaR mediante la función de distribución empírica de los datos. **HS** utiliza los datos pasados como guía de lo que va a pasar en el futuro. 

#### Algoritmo de estimación: 

1. Escoja una secuencia pasada de 𝒎 retornos diarios hipotéticos. 

2. Construya de la función de pérdidas: los retornos son ordenados de mayor a menor y multiplicados por menos uno. 
3. El **VaR** es igual a: $VaR_{(1-\alpha)}=q_{(1-\alpha)}(L)$, donde $q_{(1-\alpha)}$ es el percentil $(1-\alpha)$ de la función de pérdidas definida como $L$.

4. El **ES** es igual al promedio de las pérdidas que se encuentran por encima del **VaR**.

#### Estimación

Cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
library(rugarch)
library(qrmtools)
library(MTS)
library(ismev)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2016/01/01')
endd=c('2021/12/01')

```
Descargamos el SP500:

```{r message=FALSE}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
```
Calculamos los retornos logarítmicos:

```{r warning=FALSE}
ret <-diff(log(sp))
ret <-na.omit(ret)
```
Ahora definimos la ventana de tiempo que vamos a utilizar ($m$), el nivel de confianza $(1-\alpha)$ y la función de pérdidas. Recuerden que entre más grande sea la ventana de tiempo mayor persistencia tendrá la medida de riesgo estimada.

```{r}
m = nrow(ret)
alpha = 0.05
L = ret*-1
```

Así, el **VaR** por **HS** es igual a:

```{r}
VaRHist = quantile(L,(1-alpha))
VaRHist = as.numeric(VaRHist)
```

El **ES** por **HS** sería igual a:

```{r}
L <- as.data.frame(L)
ESHist <- filter(L,L>VaRHist)%>%
  summarise(mean(`GSPC.Adjusted`))
ESHist <- as.numeric(ESHist)

```

Para las siguientes tres metodologías debemos hacer la distinción entre un VaR/ES **estático** y uno **condicional**. El primero asume la media y la varianza de los retornos como constantes durante el período de análisis, mientras que el segundo asume que son dinámicos. 

#### Cálculo Media

Primero, calculamos la media estática:

```{r}
L = ret*-1
miu = mean(L)
```
Segundo, calculamos la media condicional, que es el pronóstico del día siguiente que obtenemos de un modelo ARIMA:

```{r}
auto.arima(L)
modelo.retornos = arima(ret, order=c(1,0,4))
coeftest(modelo.retornos)
forecast<- predict(modelo.retornos, n.ahead=1)

miut1 <- as.numeric(forecast[["pred"]])
```
#### Cálculo Volatilidad

Primero, calculamos la volatilidad estática:

```{r}
sigma = sqrt(var(L))
```
Segundo, calculamos la volatilidad condicional, que es el pronóstico del día siguiente que obtenemos de un modelo GARCH o EWMA. Usualmente, un GARCH(1,1) es suficiente:

```{r}
res = residuals(modelo.retornos)
modelo.vol <- garch(res,order=c(1,1))
summary(modelo.vol)

forecast.vol <- predict(modelo.vol,n.head=1) 

sigmat1 <- as.numeric(sqrt(forecast.vol[2,1])) 
```
### Método de Normalidad

#### Estático

Al ser el **VaR** un cuantil de la función de pérdidas, una forma de calcularlo paramétricamente es suponiendo que ésta sigue una **distribución normal**: 

$VaR_{(1-\alpha)}=\mu + \sigma\Phi^{-1}(1-\alpha)$

Donde $(1-\alpha)$ es el nivel de confianza, $\Phi^{-1}$ es la inversa generalizada de la función de distribución normal estándar (o función cuantil), $\mu$ es una estimación de la media y $\sigma$ de la desviación estándar.

Así, el **VaR estático** por el método de normalidad sería igual a: 

```{r}
alpha = 0.05
z <- qnorm((1-alpha),mean = 0, sd = 1)  

VaRNormEst <- miu + sigma*z
VaRNormEst <- as.numeric(VaRNormEst)
```

Análogamente al caso del VaR, se tiene que el **ES** por este método es igual a:

$ES_{(1-\alpha)}=\mu + \sigma\frac{\phi(\Phi^{-1}(1-\alpha))}{\alpha}$

Donde $(1-\alpha)$ es el nivel de confianza, $\Phi^{-1}$ es la inversa generalizada de la función de distribución normal estándar o la función cuantil, y $\phi$ es la función de densidad normal estándar.

Entonces, con nuestros datos el **ES estático** sería igual a:

```{r}
df <- dnorm(z)

ESNormEst <-miu + sigma*(df/alpha)  
ESNormEst <- as.numeric(ESNormEst)
```

#### Condicional 

Es posible mejorar la estimación del método bajo normalidad mediante cálculos más refinados del pronóstico de la media y la varianza, generalmente a través de modelos **ARIMA** para la media y **EWMA** o **GARCH** para la varianza. 

De esta forma se obtiene un **pronóstico condicional** para el cálculo del **VaR** que está dado por:

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}\Phi^{-1}(1-\alpha)$

Donde el subíndice de ${t+1}$ implica que el VaR es condicional a las estimaciones para el período ${t+1}$.

Así, el **VaR condicional** por el método de normalidad sería igual a: 

```{r}
alpha = 0.05
z <- qnorm((1-alpha),mean = 0, sd = 1)  

VaRNormCon <- miut1 + sigmat1*z
```
La versión **condicional** del **ES** también consiste en reemplazar la media y la varianza por los momentos condicionados a la información disponible hasta el período $t$:

$ES_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}\frac{\phi(\Phi^{-1}(1-\alpha))}{\alpha}$

Entonces, con nuestros datos el **ES condicional** sería igual a:

```{r}
df <- dnorm(z)

ESNormCon <-miut1 + sigmat1*(df/alpha)  
```

### Método de Simulación Montecarlo

Este método hace uso de la **simulación** de una distribución teórica **paramétrica** para realizar la estimación del riesgo. El método consiste en la elección de una función de pérdidas teórica y la calibración de ésta de acuerdo con los datos históricos de los factores de riesgo.

#### Algoritmo de Estimación: 

1. Elección de la función teórica sobre la cual se realiza la simulación.

2. Simulación de números aleatorios que se distribuyen según la distribución teórica escogida (usualmente el número de réplicas que se realiza es mayor que el número de datos de cada factor de riesgo, para lograr una mayor precisión en la estimación).

3. Construcción de una “función de pérdidas” con los datos simulados, análoga al método histórico.

4. Estimación del percentil asociado con el nivel de significancia 𝜶 escogido para el VaR, mediante el uso de la función de pérdidas.

Cuando la función teórica escogida es la **normal**, el **VaR** bajo este método se resume como:

$VaR_{(1-\alpha)}=\mu + \sigma(q_{(1-\alpha)}(Z))$ donde $Z\sim N(0,1).$

```{r}
x1 <- rnorm(1000, mean = 0, sd = 1)
L1 <- x1*-1

VaRMontEst = miu + sigma*quantile(L1,(1-alpha))
```

La versión **dinámica** consiste en reemplazar los pronósticos estáticos de la media y la desviación estándar por sus versiones condicionales:

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}(q_{(1-\alpha)}(Z))$ donde $Z\sim N(0,1).$

```{r}
VaRMontCon = miut1 + sigmat1*quantile(L1,(1-alpha))
```

El **ES** se calcula obteniendo el promedio de los retornos por encima del VaR (como en el caso de la simulación histórica), pero también podemos obtener una medida estática o condicional. 

```{r}
umbral <- as.numeric(quantile(L1,(1-alpha)))
L1 <- as.data.frame(L1)

q <- filter(L1,L1>umbral)%>%
  summarise(mean(`L1`))

ESMontEst <- miu + sigma*q[1]
ESMontEst <- as.numeric(ESMontEst)

ESMontCon <- miut1 + sigmat1*q[1]
ESMontCon <- as.numeric(ESMontCon)

```
### Método de Teoría del Valor Extremo

La **Teoría del Valor Extremo (EVT)** permite calcular un VaR más acorde con algunos hechos estilizados de las series financieras, en particular con la forma leptocúrtica de las colas de la función de pérdidas. Siguiendo la metodología de **POT (peaks over the threshold)**, los valores extremos que están por encima de cierto umbral (los retornos que están en las colas), se distribuyen de acuerdo a una distribución **Generalizada de Pareto (GDP)**. 

Para este caso la ecuación del **VaR** es igual a: 

$VaR_{(1-\alpha)}=q_{(1-\alpha)}(Z)=\upsilon+\frac{\beta}{\xi}\left[(\frac{\alpha}{T_{\upsilon}/T})^{-\xi}-1\right]$

Donde $(1-\alpha)$ es el nivel de confianza, $\upsilon$ es el umbral, $\beta$ es el parámetro de escala, $\xi$ es el parámetro de forma, $T$ es el número total de observaciones, y $T_{\upsilon}$ es el número de valores extremos. 

Así, el **VaR estático y condicional** son iguales a: 

$VaR_{(1-\alpha)}=\mu + \sigma q_{(1-\alpha)}(Z)$ 

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1} q_{(1-\alpha)}(Z)$

Para estimar el **VaR** primero debemos partir de datos i.id, es decir, debemos trabajar con los residuales del modelo ARIMA estandarizados por la volatilidad obtenida del modelo en varianza. 

```{r}
res.garch <- fitted.values(modelo.vol)
res.garch <- res.garch[2:m,1]
res <- res[2:m]

res.est <- res/res.garch

```
Ahora procedemos a modelar las colas de la distrubición de los residuales estandarizados:

```{r}
tailFraction <- 0.05 

TVE <- gpd.fit(res.est, threshold=quantile(res.est, (1-tailFraction)))

beta <- TVE[["mle"]][1] #Escala
eps  <- TVE[["mle"]][2] #Forma

v    <- quantile(res.est,(1-tailFraction))
N    <- length(res.est)  
Tv   <- round(N*tailFraction) 
```
Así, el *VaR* estático y condiconal con la metodología de *TVE* sería igual a:

```{r}
qTVE   <- v + (beta/eps)*((((1-tailFraction)/(Tv/N))^(-eps))-1)

VaRTVEEst <- miu + sigma*qTVE  

VaRTVECon <- miut1 + sigmat1*qTVE       
```
De acuerdo con las estimaciones realizadas de la distribución en las colas, el **ES** es igual a: 

$ES_{(1-\alpha)}=\frac{q_{(1-\alpha)}}{1-\xi} + \frac{\beta-\xi\upsilon}{1-\xi}$

Donde $(1-\alpha)$ es el nivel de confianza, $\upsilon$ es el umbral, $\beta$ es el parámetro de escala, $\xi$ es el parámetro de forma. 

El caso **condicional** sería igual a:

$ES_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}ES_{(1-\alpha)}$

Con nuestros datos:

```{r}
ESTVE <- (qTVE/(1-eps))+((beta-eps*v)/(1-eps))

ESTVEEst <- miu + sigma*ESTVE
ESTVEEst <- as.numeric(ESTVEEst)

ESTVECon <- miut1 + sigmat1*ESTVE
```

#### Resumen

```{r}
ResumenVaR <- rbind(VaRHist*100,VaRNormEst*100,VaRNormCon*100,VaRMontEst*100,VaRMontCon*100,VaRTVEEst*100,VaRTVECon*100)

ResumenES <- rbind(ESHist*100,ESNormEst*100,ESNormCon*100,ESMontEst*100,ESMontCon*100,ESTVEEst*100,ESTVECon*100)

Resumen <- cbind(ResumenVaR,ResumenES) 

rownames(Resumen) <- c("Histórico","Normalidad Estático","Normalidad Condicional","Montecarlo Estático","Montecarlo Condicional","TVE Estático","TVE Condicional")
colnames(Resumen) <- c("VaR","ES")

Resumen %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")
```

## VaR y Backtesting de un Portafolio

Aunque modelar el riesgo agregado de un portafolio directamente es útil para realizar una gestión del riesgo pasiva, **no es útil para una gestión activa**. Si queremos hacer **análisis de sensibilidad** para evaluar los beneficios de la diversificación, debemos modelar la dependencia entre los retornos individuales. En otras palabras,**¡Necesitamos modelar la matriz de Varianza-Covarianza!**

Al igual que en el caso de la modelación de la varianza, tenemos dos opciones para esta matriz: asumir que la **covarianza** entre los activos del portafolios es constante en el tiempo o que es **dinámica**. 

En el siguiente ejemplo, vamos a calcular la matriz de varianza-covarianza haciendo uso de la metodlogía propuesta por Engle en 2002 llamada *Dynamic Conditional Correlation (DCC)*. Bajo esta metodología se modela la **correlación** en vez de la varianza. Esto está motivado por la necesidad de eliminar la restricción de que varianzas y covarianzas deban tener los mismos parámetros de persistencia. 

Empíricamente, se ha evidenciado que las correlaciones se incrementan en momentos de incertidumbre financiera haciendo que el riesgo se incremente aún más. 

Así, primero debemos estimar la volatilidad condicional de cada activo a través de un modelo GARCH o EWMA. Luego, estandarizamos cada retorno por su desviación dinámica. Posteriormente, calculamos la correlación condicional entre cada par de activos, considerando una especificación de tipo GARCH(1,1): 

$q_{ij,t+1}=p_{ij}+\alpha(z_{i,t}z_{j,t}-p_{ij})+\beta(q_{ij,t}-p_{ij})$

donde la dinámica de las correlaciones está descrita por $q_{ij,t+1}$, y ésta se actualiza con los retornos estandarizados $z_{i,t}$ y $z_{j,t}$.

### DCC-GARCH

Primero cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(rmgarch)
library(FinTS)
library(rugarch)
library(e1071)
```

Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Ahora vamos a descargar el S&P 500 y las primeras 10 acciones del índice con mayor capitalización bursátil a enero de 2022: 

```{r warning=FALSE, message=FALSE}
#S&P500
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp1<-na.locf(GSPC$GSPC.Adjusted)  

#1.Apple
getSymbols("AAPL",src='yahoo', from= startt , to = endd);sp2<-na.locf(AAPL$AAPL.Adjusted) 

#2.Microsoft
getSymbols("MSFT",src='yahoo', from= startt , to = endd); sp3<-na.locf(MSFT$MSFT.Adjusted)  

#3.Amazon
getSymbols("AMZN",src='yahoo', from= startt , to = endd); sp4<-na.locf(AMZN$AMZN.Adjusted)  

#4.Alphabet Class A
getSymbols("GOOGL",src='yahoo', from= startt , to = endd); sp5<-na.locf(GOOGL$GOOGL.Adjusted)

#5.Tesla
getSymbols("TSLA",src='yahoo', from= startt , to = endd); sp6<-na.locf(TSLA$TSLA.Adjusted) 

#6.Alphabet Class C
getSymbols("GOOG",src='yahoo', from= startt , to = endd); sp7<-na.locf(GOOG$GOOG.Adjusted)

#7.Meta
getSymbols("FB",src='yahoo', from= startt , to = endd); sp8<-na.locf(FB$FB.Adjusted) 

#8.NVIDIA Corporation
getSymbols("NVDA",src='yahoo', from= startt , to = endd); sp9<-na.locf(NVDA$NVDA.Adjusted)

#9.Berkshire Hathaway Inc
getSymbols("BRK-B",src='yahoo', from= startt , to = endd); sp10<-na.locf(`BRK-B`$`BRK-B.Adjusted`) 

#UnitedHealth Group
getSymbols("UNH",src='yahoo', from= startt , to = endd); sp11<-na.locf(UNH$UNH.Adjusted)

#Tbill 13 weeks
getSymbols("^IRX",src='yahoo', from= startt , to = endd); sp12<-na.locf(IRX$IRX.Adjusted)     
```
En la siguiente parte unimos todas las series en una misma matriz y obtenemos los retornos:

```{r}
sp <- merge(sp1,sp2,fill=na.locf)
sp <- merge(sp,sp3,fill=na.locf)
sp <- merge(sp,sp4,fill=na.locf)
sp <- merge(sp,sp5,fill=na.locf)
sp <- merge(sp,sp6,fill=na.locf)
sp <- merge(sp,sp7,fill=na.locf)
sp <- merge(sp,sp8,fill=na.locf)
sp <- merge(sp,sp9,fill=na.locf)
sp <- merge(sp,sp10,fill=na.locf)
sp <- merge(sp,sp11,fill=na.locf)

sm<-sp

ret<- diff(log(sm))*100
ret<-na.locf(ret)
ret[is.na(ret)] <- 0
```
Vamos a conformar un portafolio con estas 10 acciones, asumiendo que cada una tiene el mismo peso. Nuestro objetivo es obtener la volatilidad del portafolio haciendo uso de la metodología **DCC-GARCH**.

**Paso 1: Modelación GARCH**

```{r}
uspec = ugarchspec(mean.model=list(armaOrder=c(0,0)),
                   variance.model=list(garchOrder=c(1,1),
                   model="sGARCH"),distribution.model="std")

```

**Paso 2: Espeficación DCC**

```{r}
n = ncol(ret)-1

spec1= dccspec(uspec=multispec(replicate(n,uspec)),
               dccOrder=c(1,1),distribution="mvt")
```

**Paso 3: Estimación**

```{r}
ret = ret[,2:11]

fit = dccfit(spec1,data=ret)
```

**Paso 4: Pronóstico**

```{r}
Pron = dccforecast(fit,n.ahead=1)

H = Pron@model[["H"]]
```

### Portafolio

Ahora calculamos la volatilidad del portafolio de 10 acciones,a partir de la matriz de varianza-covarianza obtenida por el modelo DCC-GARCH: 

```{r}
t=nrow(ret)

sigmaP = matrix(0,t,1)
w = matrix(0.1,1,n) #1x10
wT =matrix(0.1,n,1) #10x1

for (i in 1:t) {
  Hd=H[,,i]
  wH = w%*%Hd
  wHwT = wH%*%wT
  sigmaP[i,1]=sqrt(wHwT)
}
```
Los retornos de este portafolio con igual peso para todas las acciones serían igual a:

```{r}
RetP = matrix(,t,1)
w = as.vector(w)

for (i in 1:t) {
  retd = as.vector(ret[i,])
  RetP[i,1] = crossprod(retd,w)
}
```
Los retornos estandarizados por el modelo de volatilidad estimado serían igual a:

```{r}
Sigmas=fit@model[["sigma"]]

RetS = ret/Sigmas

RetPS = matrix(,t,1)
w = as.vector(w)

for (i in 1:t) {
  retsd = as.vector(RetS[i,])
  RetPS[i,1] = crossprod(retsd,w)
}
```
### Estimación del VaR

Ahora calculamos el VaR del portafolio por el método de normalidad y el histórico a un día con un nivel de confianza del 99%.

#### VaR Normalidad

```{r}
conf = 0.99

q= qnorm(conf,mean=0,sd=1)

VaRN = matrix(,t,1)

for (i in 1:t) {
  VaRN[i,1]=sigmaP[i,1]*q
}
```
#### VaR Histórico

```{r}
m=t-249
VaRH = matrix(,m,1)

for (i in 1:m) {
  VaRH[i,1] = quantile(RetP[i:(249+i),1],probs=0.01)*-1
}
```

### Backtesting

El backtesting consiste en probar si efectivamente se cumple que los retornos (o residuales estandarizados) sobrepasan el pronóstico de la medida del riesgo el $\alpha*100$% del tiempo, como lo promete el $VaR_{(1-\alpha),t+1}$  (*unconditional coverage test*). 

Además, también se debe probar si las observaciones que exceden al VaR no están agrupadas (*independence test*). 

A continuación, se presenta el Backtesting de los VaR calculados anteriormente. 

#### Backtesting VaR Normalidad 

```{r}
t2=t-1

VaRN2 = VaRN[1:t2,1]
RetPS2 = RetPS[2:t,1]
Backtest = matrix(,2,4)

Backtest[1,1]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$expected.exceed
Backtest[1,2]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$actual.exceed
Backtest[1,3]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$uc.Decision
Backtest[1,4]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$cc.Decision

```

#### Backtesting VaR Histórico

```{r}
VaRH2 = VaRH[1:(m-1)]
RetP3 = RetP[251:t,1]

Backtest[2,1]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$expected.exceed
Backtest[2,2]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$actual.exceed
Backtest[2,3]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$uc.Decision
Backtest[2,4]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$cc.Decision

```

#### Resumen

```{r}
rownames(Backtest) <- c("DCC-GARCH Normal","Historical")
colnames(Backtest) <- c("Expected exceedances","Actual exceedances","uc.Decision","cc.Decision")

Backtest %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")
```

# Riesgo de Crédito

El **riesgo de crédito** se define como la pérdida potencial que se registra con motivo del incumplimiento de una contraparte en una transacción financiera. También se concibe como un **deterioro** en la calidad crediticia de la contraparte o en la garantía o colateral pactada originalmente. Tradicionalmente, los bancos y empresas han elaborados procedimientos o normas de crédito para seleccionar los clientes a quienes prestarles, montos y tasas. 

Los modelos más utilizados en el mercado para medir riesgo de crédito son: 

1. **Modelos econométricos:** se utilizan modelos de regresión múltiple y modelos Logit y Probit para determinar las probabilidades de default. Las variables independientes son indicadores financieros o características del cliente, y variables externas que miden el comportamiento macroeconómico.
 
2. **Modelo de Merton y Moody’s KMV:** se basa en la teoría de opciones para calcular la probabilidad de default y el valor de la deuda. 

A continuación veremos ejemplos de diferentes modelos econométricos para calcular la probabilidad de default de un cliente de un banco. 

Cargamos las librerías que vamos a utilizar:

```{r, warning=FALSE,message=FALSE}
library(caret)
library(dplyr)
library(e1071)
library(MASS)
library(MLmetrics)
library(readr)
library(rpart)
library(UBL)
library(kableExtra)
```

Importamos la base de datos crediticia y especificamos las variables categóricas como factores: 

```{r}
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=FALSE)

dd$Status = as.factor(dd$Status)
dd$Home = as.factor(dd$Home)
dd$Marital = as.factor(dd$Marital)
dd$Records = as.factor(dd$Records)
dd$Job = as.factor(dd$Job)

summary(dd)
```

Ahora dividimos los datos en 2/3 para el aprendizaje del modelo y 1/3 para la validación de éste.

```{r}
set.seed(100)
n <- nrow(dd)
learn <- sample(1:n, size=round(0.67 * n))
train <- dd[learn,]
test  <- dd[-learn,]
```

## Regresión Logística

```{r}
set.seed(10116070)

logmodel = glm(Status~., train, family = "binomial")
summary(logmodel)
prob_logpred = predict(logmodel,test,type = "response")
logpred = ifelse(prob_logpred > 0.5, "1", "0") 

confmatrix <- table(logpred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionLR <- (TP/(TP+FP))
RecallLR <- (TP/(TP+FN))
F1LR <- 2*(PrecisionLR*RecallLR/(PrecisionLR+RecallLR))
AccuracyLR <- (TP+TN)/(TN+TP+FN+TP)

SummaryLR <- rbind(PrecisionLR,RecallLR,F1LR,AccuracyLR)
```

## Support Vector Machine

```{r}
set.seed(10116070)
svm_model <- svm(Status ~ ., data=train)
svm_pred <- predict(svm_model,test)

confmatrix <- table(svm_pred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionSVM <- (TP/(TP+FP))
RecallSVM <- (TP/(TP+FN))
F1SVM <- 2*(PrecisionSVM*RecallSVM/(PrecisionSVM+RecallSVM))
AccuracySVM <- (TP+TN)/(TN+TP+FN+TP)

SummarySVM <- rbind(PrecisionSVM,RecallSVM,F1SVM,AccuracySVM)
```

## Árbol de Decisión

```{r}
set.seed(10116070)
tree_model = rpart(Status ~ ., data = train, method = 'class')
tree_pred = as.data.frame(predict(tree_model,test))
tree_pred = tree_pred[,2]
tree_pred = ifelse(tree_pred > 0.5, "1", "0") 

confmatrix <- table(tree_pred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionDT <- (TP/(TP+FP))
RecallDT <- (TP/(TP+FN))
F1DT <- 2*(PrecisionDT*RecallDT/(PrecisionDT+RecallDT))
AccuracyDT <- (TP+TN)/(TN+TP+FN+TP)

SummaryDT <- rbind(PrecisionDT,RecallDT,F1DT,AccuracyDT)
```

## Nearest Neighbour

```{r}
set.seed(10116070)
trCtrl = trainControl(method = "repeatedcv", number = 10, repeats = 3 )
nnmodel = train(Status~.,data=train,method = 'knn',tuneLength = 20, trControl = trCtrl)
nnpred = predict(nnmodel,test)
table(nnpred,test$Status)

confmatrix <- table(nnpred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionNN <- (TP/(TP+FP))
RecallNN <- (TP/(TP+FN))
F1NN <- 2*(PrecisionNN*RecallNN/(PrecisionNN+RecallNN))
AccuracyNN <- (TP+TN)/(TN+TP+FN+TP)

SummaryNN <- rbind(PrecisionNN,RecallNN,F1NN,AccuracyNN)
```

#### Resumen

```{r}
Resumen <- cbind(SummaryLR,SummarySVM,SummaryDT,SummaryNN)

rownames(Resumen) <- c("Precision","Recall","F1","Accuracy")
colnames(Resumen) <- c("Logist Regression","Support Vector Machine",
                       "Decision Tree","Nearest Neighbour")

Resumen %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```

# Riesgo Operacional

## ¿Qué es el riesgo operacional?

Es una categoría de riesgo amplia que incluye desde la posibilidad de fallas en los procesos y fraudes hasta desastres naturales. En otras palabras, tener muchos días realmente malos. 

Para medir el riesgo operacional debemos calcular la `frecuencia` y `severidad`. Existe una analogía al riesgo de crédito, donde la frecuencia es es la probabilidad de default y la severidad es la proporción no recuperable de la deuda. En riesgo operacional, pensamos en la probabilidad de una pérdida como con qué frecuencia puede ocurrir esta pérdida. 

En cuanto a la severidad, pensamos en la pérdida monetaria. Estas pérdidas suelen estar por encima de un `threshold`, por lo que debemos enfocarnos en los valores extremos. 

En la mayoría de las organizaciones no existe series de tiempo o de corte transversal con buena información sobre los eventos de riesgo operacional. Esto se debe principalmente a que muchos eventos de riesgo operacional nunca han sucedido antes. 

#### Un ejemplo

Suponga que la administración de una empresa, después de mucha discusión con expertos, determina que: 

1. El promedio de las pérdidas debido a ataques cibernéticos es de $60 millones
2. Con una frecuencia promedio del 25% 
3. En un período de 12 meses
4. La desviación estándar de las estimaciones es de $20 millones

### ¿Cuánto puedo perder?

Los managers pueden utilizar diferentes distribuciones de probabilidad para expresar sus preferencias sobre el riesgo de una pérdida. Una de estas es la distribución `gamma` como función de severidad. Esta distribución permite colas pesadas y asimetría. 

La distribución `gamma` está especificada sólo con los dos primeros momentos: media y varianza. La probabilidad de que una pérdida sea igual a $x$ es:

$$p(x)= \frac{\beta^{\alpha}x^{\alpha-1}e^{-x\beta}}{\Gamma(\alpha)}$$
donde $\beta=\mu/\sigma^{2}$ y $\alpha=\mu\beta$.

Utilicemos entonces esta distribución en nuestro ejemplo para simular la severidad de las pérdidas:

```{r warning=TRUE}
set.seed(1004)
n.sim <- 1000
mu <- 60     ## el promedio
sigma <- 20  ## qué tan inciertas piensa la administración que son las estimaciones 
sigma.sq <- sigma^2
beta <- mu/sigma.sq
alpha <- beta * mu
severity <- rgamma(n.sim, alpha, beta)
summary(severity)
```


La distribución se dispersa desde un mínimo de 12 millones a un máximo de más de 150 millones. Asumiendo que la adminsitración piensa que este mínimo y máximo son razonables, gráfiquemos la distribución. 

Creamos un data frame en `gamma.sev` para crear los valores a utilizar en el `ggplot`:

```{r warning=TRUE}
library(ggplot2)
gamma.sev <- data.frame(Severity = severity, 
    Distribution = rep("Gamma", each = n.sim))
ggplot(gamma.sev, aes(x = Severity, fill = Distribution)) + 
    geom_density(alpha = 0.3)
```

Podemos agregar thresholds basados en el valor en riesgo (`VaR`) y el expected shortfall (`ES`). Calculamos las dos medidas de riesgo para un nivel de confianza de $1-\alpha$.

```{r}
alpha.tolerance <- 0.05

(VaR.sev <- quantile(severity, 1 - alpha.tolerance))

(ES.sev <- mean(gamma.sev$Severity[gamma.sev$Severity > VaR.sev]))
```

Grafiquemos el VaR y el ES:

```{r}
ggplot(gamma.sev, aes(x = Severity, fill = Distribution)) + 
    geom_density(alpha = 0.3) + geom_vline(xintercept = VaR.sev, 
    color = "red") + geom_vline(xintercept = ES.sev, 
    color = "blue")
```

### ¿Qué tan seguido ocurren mis pérdidas?

Usualmente se modela la frecuencia de las pérdidas con una distribución `poisson`. Es una distribución que está definida por el parámetro $\lambda$, es decir, por el número de veces que se espera que ocurra el evento durante un intervalo dado de tiempo.

La probabilidad de que $x$ eventos de riesgo ocurran a una tasa de llegada de $\lambda$ es igual a: 

$$p(x)= \frac{\lambda^{x}e^{-\lambda}}{x!}$$
La administración asume que $\lambda=0.25$. Así, podemos decir que se cree que un evento de ataque cibernético ocurrirá una vez cada 4 meses en los próximos 12 meses. 

Vamos a simular `n.sim` eventos con una distribuicón `rpois()` condicional a $\lambda=0.25$:

```{r}
n.sim <- 1000
lambda <- 0.25
frequency <- rpois(n.sim, lambda)
summary(frequency)
```

También podemos graficar la distribución:

```{r}
poisson.freq <- data.frame(Frequency = frequency, 
    Distribution = rep("Poisson", each = n.sim))

ggplot(poisson.freq, aes(x = frequency, 
    fill = Distribution)) + geom_density(alpha = 0.3)
```
La gráfica nos muestra una versión suavizada del recuento de eventos discretos: la cantidad de eventos más probable es cero, luego un evento, luego dos, luego tres eventos.

### ¿Cuál es la pérdida potencial?

Para calcular la pérdida potencial debemos combinar frecuencia y severidad. Necesitamos "convolucionar" las distribuciones de frecuencia y severidad para formar una distribución de pérdidas. “Convolucionar” significa que para cada escenario de pérdida de dólares cibernéticos simulado, vemos si el escenario ocurre o no, usando los escenarios de frecuencia de Poisson.

```{r warning=TRUE}
loss <- rpois(n.sim, severity * frequency)
summary(loss)
```

Algunas anotaciones:

1. Este código toma cada una de las severidades distribuidas `gamma` (`n.sim` de ellas) y se pregunta con qué frecuencia ocurrirá cada una de ellas (`lambda`).

2. Luego simula la frecuencia de estos eventos de riesgo escalados por la severidad `gamma` del tamaño de la pérdida.

3. El resultado es una convolución de todos las `n.sim` de las severidades con todas las `n.sim` de las frecuencias. 

Calculemos ahora las medidas de riesgo para las pérdidas potenciales:

```{r warning=TRUE}
loss.rf <- data.frame(Loss = loss, Distribution = rep("Potential Loss", 
    each = n.sim))

(VaR.loss.rf <- quantile(loss.rf$Loss, 1 - alpha.tolerance))

(ES.loss.rf <- mean(loss.rf$Loss[loss.rf$Loss > VaR.loss.rf]))
```
Grafiquemos:

```{r}
ggplot(loss.rf, aes(x = Loss, fill = Distribution)) + 
    geom_density(alpha = 0.3) + geom_vline(xintercept = VaR.loss.rf, 
    color = "red") + geom_vline(xintercept = ES.loss.rf, 
    color = "blue")
```

